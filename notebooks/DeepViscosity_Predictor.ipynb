{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqYhZfJnSvsH"
      },
      "source": [
        "## Install and Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icCMgpaXYyFN"
      },
      "outputs": [],
      "source": [
        "# Installation for Colab environment. If not in Colab, manage environment separately.\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab. Installing dependencies...\")\n",
        "    !pip install -q condacolab\n",
        "    import condacolab\n",
        "    condacolab.install()\n",
        "    !conda install -c bioconda anarci -y\n",
        "    !pip install joblib dill\n",
        "    # Pinned versions for potential Colab compatibility, adjust as needed for local env.\n",
        "    !pip install keras==2.11.0 tensorflow==2.11.0 scikit-learn==1.0.2\n",
        "else:\n",
        "    print(\"Not running in Colab. Ensure ANARCI and other dependencies are installed in your environment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOqM10kcxHb9"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import subprocess\n",
        "import random\n",
        "import logging\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.optimizers import Adam # Using tf.keras.optimizers\n",
        "# from tensorflow.keras.utils import plot_model # Optional: if model plotting is needed\n",
        "\n",
        "from Bio import SeqIO\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "\n",
        "# --- Configuration and Constants ---\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
        "\n",
        "NOTEBOOK_DIR = os.path.abspath('')\n",
        "BASE_PROJECT_DIR = os.path.dirname(NOTEBOOK_DIR)\n",
        "BASE_DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'data')\n",
        "INPUT_DIR = os.path.join(BASE_DATA_DIR, 'input')\n",
        "DEEPSP_MODEL_DIR = os.path.join(BASE_DATA_DIR, 'DeepSP_CNN_model')\n",
        "DEEPVISCOSITY_SCALER_DIR = os.path.join(BASE_DATA_DIR, 'DeepViscosity_scaler')\n",
        "DEEPVISCOSITY_MODEL_DIR = os.path.join(BASE_DATA_DIR, 'DeepViscosity_ANN_ensemble_models')\n",
        "TEMP_OUTPUT_DIR = os.path.join(NOTEBOOK_DIR, 'temp_outputs') # For notebook-specific temporary files\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "os.makedirs(TEMP_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper_functions_markdown"
      },
      "source": [
        "## Helper Functions (Adapted from script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper_functions_code"
      },
      "outputs": [],
      "source": [
        "def create_fasta_file_nb(sequences: list[str], names: list[str], file_path: str):\n",
        "    logging.info(f\"Creating FASTA file: {file_path}\")\n",
        "    with open(file_path, \"w\") as output_handle:\n",
        "        for i, seq_str in enumerate(sequences):\n",
        "            record = SeqRecord(Seq(seq_str), id=names[i], name=\"\", description=\"\")\n",
        "            SeqIO.write(record, output_handle, \"fasta\")\n",
        "    logging.info(f\"FASTA file created: {file_path}\")\n",
        "\n",
        "def run_anarci_nb(input_fasta_path: str, output_base_name: str, chain_type: str):\n",
        "    logging.info(f\"Running ANARCI for {chain_type} chain: {input_fasta_path}\")\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['ANARCI', '-i', input_fasta_path, '-o', output_base_name, '-s', 'imgt', '-r', chain_type, '--csv'],\n",
        "            check=True, capture_output=True, text=True\n",
        "        )\n",
        "        logging.info(f\"ANARCI alignment successful for {chain_type} chain. Output files prefixed with: {output_base_name}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logging.error(f\"ANARCI failed for {chain_type} chain. Error: {e.stderr}\")\n",
        "        raise\n",
        "\n",
        "def preprocess_aligned_sequences_nb(h_aligned_path: str, l_aligned_path: str, outfile_path: str):\n",
        "    logging.info(f\"Preprocessing aligned sequences. H: {h_aligned_path}, L: {l_aligned_path} -> {outfile_path}\")\n",
        "    try:\n",
        "        infile_H = pd.read_csv(h_aligned_path)\n",
        "        infile_L = pd.read_csv(l_aligned_path)\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"Error reading ANARCI output files: {e}\")\n",
        "        raise\n",
        "\n",
        "    with open(outfile_path, \"w\") as outfile:\n",
        "        H_inclusion_list = [str(i) for i in range(1, 129)] + \\\n",
        "                           ['111A','111B','111C','111D','111E','111F','111G','111H',\n",
        "                            '112I','112H','112G','112F','112E','112D','112C','112B','112A']\n",
        "        L_inclusion_list = [str(i) for i in range(1, 128)]\n",
        "\n",
        "        h_pos_map = {\n",
        "            **{str(i): i-1 for i in range(1, 112)},\n",
        "            '111A':111,'111B':112,'111C':113,'111D':114,'111E':115,'111F':116,'111G':117,'111H':118,\n",
        "            '112I':119,'112H':120,'112G':121,'112F':122,'112E':123,'112D':124,'112C':125,'112B':126,'112A':127,\n",
        "            **{str(i): i+16 for i in range(112, 129)}\n",
        "        }\n",
        "        h_idx = 110\n",
        "        for letter_code in [''] + [chr(ord('A') + i) for i in range(9)]:\n",
        "            if f\"111{letter_code}\" in H_inclusion_list and f\"111{letter_code}\" not in h_pos_map:\n",
        "                h_pos_map[f\"111{letter_code}\"] = h_idx; h_idx+=1\n",
        "            if f\"112{letter_code}\" in H_inclusion_list and f\"112{letter_code}\" not in h_pos_map:\n",
        "                 h_pos_map[f\"112{letter_code}\"] = h_idx; h_idx+=1\n",
        "        for i in range(113,129):\n",
        "            if str(i) in H_inclusion_list and str(i) not in h_pos_map:\n",
        "                h_pos_map[str(i)] = h_idx; h_idx+=1\n",
        "\n",
        "        l_pos_map = {str(i): i-1 for i in range(1, 128)}\n",
        "\n",
        "        N_mAbs = len(infile_H[\"Id\"])\n",
        "        for i in range(N_mAbs):\n",
        "            H_tmp = 145*['-']\n",
        "            L_tmp = 127*['-']\n",
        "            for col in infile_H.columns:\n",
        "                if col in H_inclusion_list and col in h_pos_map:\n",
        "                    pos_idx = h_pos_map[col]\n",
        "                    if 0 <= pos_idx < len(H_tmp): H_tmp[pos_idx]=infile_H.iloc[i][col]\n",
        "                    else: logging.warning(f\"H-chain index {pos_idx} for col {col} out of bounds (len {len(H_tmp)}).\")\n",
        "            for col in infile_L.columns:\n",
        "                if col in L_inclusion_list and col in l_pos_map:\n",
        "                    pos_idx = l_pos_map[col]\n",
        "                    if 0 <= pos_idx < len(L_tmp): L_tmp[pos_idx]=infile_L.iloc[i][col]\n",
        "                    else: logging.warning(f\"L-chain index {pos_idx} for col {col} out of bounds (len {len(L_tmp)}).\")\n",
        "            aa_string = \"\".join(H_tmp + L_tmp)\n",
        "            outfile.write(f\"{infile_H.iloc[i,0]} {aa_string}\\n\")\n",
        "    logging.info(\"Sequence preprocessing finished.\")\n",
        "\n",
        "def load_aligned_data_nb(filename: str) -> tuple[list[str], list[str]]:\n",
        "    logging.info(f\"Loading aligned data from: {filename}\")\n",
        "    name_list, seq_list = [], []\n",
        "    try:\n",
        "        with open(filename) as datafile:\n",
        "            for line in datafile:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 2: name_list.append(parts[0]); seq_list.append(parts[1])\n",
        "                else: logging.warning(f\"Skipping malformed line in {filename}: {line.strip()}\")\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Aligned data file not found: {filename}\"); raise\n",
        "    return name_list, seq_list\n",
        "\n",
        "def one_hot_encode_sequence_nb(sequence: str) -> np.ndarray:\n",
        "    aa_dict = {'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,\n",
        "               'M':10,'N':11,'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,\n",
        "               'W':18,'Y':19,'-':20, 'X':20}\n",
        "    processed_sequence = \"\".join([s if s in aa_dict else 'X' for s in sequence])\n",
        "    encoded_seq = np.zeros((len(aa_dict)-1, len(processed_sequence)))\n",
        "    for i, char_s in enumerate(processed_sequence):\n",
        "        if char_s in aa_dict and aa_dict[char_s] < (len(aa_dict)-1):\n",
        "             encoded_seq[aa_dict[char_s], i] = 1\n",
        "    return encoded_seq\n",
        "\n",
        "def predict_deepsp_features_nb(X_encoded: np.ndarray, model_type: str) -> np.ndarray:\n",
        "    logging.info(f\"Predicting DeepSP for model: {model_type}\")\n",
        "    json_path = os.path.join(DEEPSP_MODEL_DIR, f'Conv1D_regression{model_type.upper()}.json')\n",
        "    weights_path = os.path.join(DEEPSP_MODEL_DIR, f'Conv1D_regression_{model_type.lower()}.h5')\n",
        "    try:\n",
        "        with open(json_path, 'r') as json_file: loaded_model_json = json_file.read()\n",
        "        model = model_from_json(loaded_model_json)\n",
        "        model.load_weights(weights_path)\n",
        "        model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "        predictions = model.predict(X_encoded, verbose=0)\n",
        "        logging.info(f\"DeepSP {model_type} prediction successful.\")\n",
        "        return predictions\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in DeepSP {model_type} prediction: {e}\"); raise\n",
        "\n",
        "def predict_deepviscosity_nb(df_deepsp_features: pd.DataFrame) -> pd.DataFrame:\n",
        "    logging.info(\"Predicting DeepViscosity...\")\n",
        "    X_features = df_deepsp_features.iloc[:, 1:].values\n",
        "    scaler_path = os.path.join(DEEPVISCOSITY_SCALER_DIR, \"DeepViscosity_scaler.save\")\n",
        "    try:\n",
        "        scaler = joblib.load(scaler_path)\n",
        "        X_scaled = scaler.transform(X_features)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error with scaler: {e}\"); raise\n",
        "\n",
        "    model_preds = []\n",
        "    num_models = 102\n",
        "    for i in range(num_models):\n",
        "        file_prefix = f'ANN_logo_{i}'\n",
        "        model_json_path = os.path.join(DEEPVISCOSITY_MODEL_DIR, f'{file_prefix}.json')\n",
        "        model_h5_path = os.path.join(DEEPVISCOSITY_MODEL_DIR, f'{file_prefix}.h5')\n",
        "        try:\n",
        "            with open(model_json_path, 'r') as json_file: loaded_model_json = json_file.read()\n",
        "            model = model_from_json(loaded_model_json)\n",
        "            model.load_weights(model_h5_path)\n",
        "            model.compile(optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "            pred = model.predict(X_scaled, verbose=0)\n",
        "            model_preds.append(pred)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error with ANN model {file_prefix}: {e}\"); raise\n",
        "    \n",
        "    final_pred = np.where(np.array(model_preds).mean(axis=0) >= 0.5, 1, 0)\n",
        "    logging.info(\"DeepViscosity prediction finished.\")\n",
        "    return final_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nifzJMA-TkHp"
      },
      "source": [
        "## 1. Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "gp2-sVDPSTXh"
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join(INPUT_DIR, 'DeepViscosity_input.csv')\n",
        "try:\n",
        "    dataset = pd.read_csv(dataset_path)\n",
        "    logging.info(f\"Dataset loaded successfully from {dataset_path}\")\n",
        "    display(dataset.head())\n",
        "except FileNotFoundError:\n",
        "    logging.error(f\"Input dataset not found: {dataset_path}\")\n",
        "    dataset = pd.DataFrame() # Ensure dataset is defined for subsequent cells\n",
        "\n",
        "names = dataset['Name'].to_list() if not dataset.empty else []\n",
        "heavy_seqs = dataset['Heavy_Chain'].to_list() if not dataset.empty else []\n",
        "light_seqs = dataset['Light_Chain'].to_list() if not dataset.empty else []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fasta_anarci_markdown"
      },
      "source": [
        "## 2. Create FASTA files and Run ANARCI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fasta_anarci_code"
      },
      "outputs": [],
      "source": [
        "seq_H_fasta_path_nb = os.path.join(TEMP_OUTPUT_DIR, 'seq_H.fasta')\n",
        "seq_L_fasta_path_nb = os.path.join(TEMP_OUTPUT_DIR, 'seq_L.fasta')\n",
        "anarci_H_out_csv_path_nb = os.path.join(TEMP_OUTPUT_DIR, 'seq_aligned_H.csv')\n",
        "anarci_L_out_csv_path_nb = os.path.join(TEMP_OUTPUT_DIR, 'seq_aligned_KL.csv')\n",
        "anarci_base_out_name_nb = os.path.join(TEMP_OUTPUT_DIR, 'seq_aligned') # ANARCI output prefix\n",
        "\n",
        "if names: # Proceed only if data was loaded\n",
        "    create_fasta_file_nb(heavy_seqs, names, seq_H_fasta_path_nb)\n",
        "    create_fasta_file_nb(light_seqs, names, seq_L_fasta_path_nb)\n",
        "    \n",
        "    run_anarci_nb(seq_H_fasta_path_nb, anarci_base_out_name_nb, 'heavy')\n",
        "    run_anarci_nb(seq_L_fasta_path_nb, anarci_base_out_name_nb, 'light')\n",
        "else:\n",
        "    logging.warning(\"Dataset is empty. Skipping FASTA creation and ANARCI.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess_markdown"
      },
      "source": [
        "## 3. Preprocess Aligned Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_code"
      },
      "outputs": [],
      "source": [
        "aligned_HL_txt_path_nb = os.path.join(TEMP_OUTPUT_DIR, 'seq_aligned_HL.txt')\n",
        "if names and os.path.exists(anarci_H_out_csv_path_nb) and os.path.exists(anarci_L_out_csv_path_nb):\n",
        "    preprocess_aligned_sequences_nb(anarci_H_out_csv_path_nb, anarci_L_out_csv_path_nb, aligned_HL_txt_path_nb)\n",
        "else:\n",
        "    logging.warning(\"Skipping sequence preprocessing due to missing ANARCI outputs or empty dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "one_hot_markdown"
      },
      "source": [
        "## 4. Load Aligned Data and One-Hot Encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "one_hot_code"
      },
      "outputs": [],
      "source": [
        "X_one_hot_nb = np.array([]) # Initialize to avoid NameError if skipped\n",
        "loaded_names_nb = []\n",
        "if os.path.exists(aligned_HL_txt_path_nb):\n",
        "    loaded_names_nb, loaded_seqs_nb = load_aligned_data_nb(aligned_HL_txt_path_nb)\n",
        "    logging.info(\"One-hot encoding sequences...\")\n",
        "    X_encoded_list_nb = [one_hot_encode_sequence_nb(s) for s in loaded_seqs_nb]\n",
        "    \n",
        "    X_transposed_list_nb = [x.T for x in X_encoded_list_nb]\n",
        "    max_seq_len_nb = 0\n",
        "    if X_transposed_list_nb: max_seq_len_nb = max(x.shape[0] for x in X_transposed_list_nb)\n",
        "    \n",
        "    X_padded_list_nb = []\n",
        "    num_features_nb = 0\n",
        "    if X_transposed_list_nb: num_features_nb = X_transposed_list_nb[0].shape[1]\n",
        "\n",
        "    for x_t_nb in X_transposed_list_nb:\n",
        "        padding_length_nb = max_seq_len_nb - x_t_nb.shape[0]\n",
        "        if padding_length_nb > 0:\n",
        "            padding_array_nb = np.zeros((padding_length_nb, num_features_nb))\n",
        "            x_padded_nb = np.vstack((x_t_nb, padding_array_nb))\n",
        "        else:\n",
        "            x_padded_nb = x_t_nb\n",
        "        X_padded_list_nb.append(x_padded_nb)\n",
        "    \n",
        "    if X_padded_list_nb: X_one_hot_nb = np.asarray(X_padded_list_nb)\n",
        "    logging.info(f\"One-hot encoded data shape: {X_one_hot_nb.shape if X_one_hot_nb.size > 0 else 'Empty'}\")\n",
        "else:\n",
        "    logging.warning(\"Skipping one-hot encoding as aligned sequence file is missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-qttNLlTuT4"
      },
      "source": [
        "## 5. DeepSP Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "Pt5KeAfZy8gF"
      },
      "outputs": [],
      "source": [
        "df_deepsp_nb = pd.DataFrame() # Initialize\n",
        "if X_one_hot_nb.size > 0:\n",
        "    sap_pos_nb = predict_deepsp_features_nb(X_one_hot_nb, 'SAPpos')\n",
        "    scm_pos_nb = predict_deepsp_features_nb(X_one_hot_nb, 'SCMpos')\n",
        "    scm_neg_nb = predict_deepsp_features_nb(X_one_hot_nb, 'SCMneg')\n",
        "\n",
        "    df_deepsp_nb = pd.concat([\n",
        "        pd.DataFrame(loaded_names_nb, columns=['Name']),\n",
        "        pd.DataFrame(sap_pos_nb), pd.DataFrame(scm_neg_nb), pd.DataFrame(scm_pos_nb)\n",
        "    ], axis=1)\n",
        "    \n",
        "    # Define column names (ensure this matches the actual number of features from DeepSP models)\n",
        "    num_sap_f = sap_pos_nb.shape[1] if sap_pos_nb.ndim > 1 else 1\n",
        "    num_scm_neg_f = scm_neg_nb.shape[1] if scm_neg_nb.ndim > 1 else 1\n",
        "    num_scm_pos_f = scm_pos_nb.shape[1] if scm_pos_nb.ndim > 1 else 1\n",
        "    \n",
        "    deepsp_cols_nb = ['Name'] + \\\n",
        "                     [f'SAP_pos_{i+1}' for i in range(num_sap_f)] + \\\n",
        "                     [f'SCM_neg_{i+1}' for i in range(num_scm_neg_f)] + \\\n",
        "                     [f'SCM_pos_{i+1}' for i in range(num_scm_pos_f)]\n",
        "    \n",
        "    if len(deepsp_cols_nb) == len(df_deepsp_nb.columns):\n",
        "        df_deepsp_nb.columns = deepsp_cols_nb\n",
        "    else:\n",
        "        logging.warning(\"Mismatch in DeepSP feature columns count for notebook. Using default numbered columns.\")\n",
        "\n",
        "    deepsp_descriptors_path_nb = os.path.join(BASE_DATA_DIR, 'DeepSP_descriptors_notebook.csv')\n",
        "    df_deepsp_nb.to_csv(deepsp_descriptors_path_nb, index=False)\n",
        "    logging.info(f\"DeepSP descriptors (notebook) saved to: {deepsp_descriptors_path_nb}\")\n",
        "    display(df_deepsp_nb.head())\n",
        "else:\n",
        "    logging.warning(\"Skipping DeepSP predictions as one-hot encoded data is not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYmarUNrT1nz"
      },
      "source": [
        "## 6. DeepViscosity Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "R0Gi5_po05Ct"
      },
      "outputs": [],
      "source": [
        "df_deepvis_nb = pd.DataFrame() # Initialize\n",
        "if not df_deepsp_nb.empty:\n",
        "    final_pred_nb = predict_deepviscosity_nb(df_deepsp_nb)\n",
        "    \n",
        "    df_deepvis_nb = pd.DataFrame({'Name': loaded_names_nb, 'DeepViscosity_classes': final_pred_nb.flatten()})\n",
        "    deepviscosity_classes_path_nb = os.path.join(BASE_DATA_DIR, 'DeepViscosity_classes_notebook.csv')\n",
        "    df_deepvis_nb.to_csv(deepviscosity_classes_path_nb, index=False)\n",
        "    logging.info(f\"DeepViscosity classes (notebook) saved to: {deepviscosity_classes_path_nb}\")\n",
        "    display(df_deepvis_nb.head())\n",
        "else:\n",
        "    logging.warning(\"Skipping DeepViscosity prediction as DeepSP features are not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new_cell_for_cleanup_id"
      },
      "source": [
        "## 7. Clean up temporary files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup_cell_id"
      },
      "outputs": [],
      "source": [
        "logging.info(f\"Attempting to clean up temporary directory: {TEMP_OUTPUT_DIR}\")\n",
        "if os.path.exists(TEMP_OUTPUT_DIR):\n",
        "    try:\n",
        "        shutil.rmtree(TEMP_OUTPUT_DIR)\n",
        "        logging.info(\"Temporary directory cleaned successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error cleaning up temporary directory {TEMP_OUTPUT_DIR}: {e}\")\n",
        "else:\n",
        "    logging.info(\"Temporary directory not found, no cleanup needed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
